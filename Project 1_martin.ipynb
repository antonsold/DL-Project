{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EE-559: Project #1\n",
    "## Classification, weight sharing, auxiliary losses\n",
    "\n",
    "- Franck Dessimoz\n",
    "- Léopold Bouraux\n",
    "- Martin Esguerra\n",
    "_____\n",
    "_____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, math, torch, time\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import dlc_practical_prologue as prologue\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "sys.path.insert(0, \"./src\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Visualize and prepare data\n",
    "\n",
    "**• Load the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "n = 1000\n",
    "train_input, train_target, train_classes, test_input, test_target, test_classes = prologue.generate_pair_sets(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**• Visualize data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensors sizes\n",
    "print('train_input size:  ', list(train_input.shape))\n",
    "print('train_target size: ', list(train_target.shape))\n",
    "print('train_classes size:', list(train_classes.shape))\n",
    "print('test_input size:   ', list(test_input.shape))\n",
    "print('test_target size:  ', list(test_target.shape))\n",
    "print('test_classes size: ', list(test_classes.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first of 1000 items for train data\n",
    "\n",
    "f = plt.figure(figsize=(10,5))\n",
    "ax1 = f.add_subplot(121)\n",
    "ax2 = f.add_subplot(122)\n",
    "f.suptitle('First pair of images from train_input')\n",
    "ax1.imshow(train_input[0][0])\n",
    "ax2.imshow(train_input[0][1])\n",
    "plt.show()\n",
    "\n",
    "print('train_target[0]:', train_target[0].tolist())\n",
    "print('train_classes[0]:', train_classes[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = train_input.mean()\n",
    "std = train_input.std()\n",
    "train_input_norm = train_input.sub(mu).div(std)\n",
    "test_input_norm = test_input.sub(mu).div(std)\n",
    "\n",
    "n_rounds = 10\n",
    "nb_params = []\n",
    "nb_errors = []\n",
    "losses_mean = []\n",
    "nb_errors_mean = []\n",
    "losses_std = []\n",
    "nb_errors_std = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_loss(model, test_in, test_out):\n",
    "    \"\"\" This function ...\n",
    "        args:\n",
    "            model:\n",
    "            test_in: \n",
    "            test_out:\n",
    "        returns:\n",
    "            loss:\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    out = model(test_in)\n",
    "    model.train()\n",
    "    loss = criterion(out, test_out).item()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_loss_inter(intermed, test_in, test_out):\n",
    "    \"\"\" This function ...\n",
    "        args:\n",
    "            intermed:\n",
    "            test_in: \n",
    "            test_out:\n",
    "        returns:\n",
    "            loss:\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    out1, out2 = intermed(test_in)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    loss1 = criterion(out1, test_out[:,0]).item()\n",
    "    loss2 = criterion(out2, test_out[:,1]).item()\n",
    "    model.train()\n",
    "    loss = loss1 + loss2\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_input, train_target,\n",
    "                test_input, test_target,\n",
    "                mini_batch_size=25, eta=0.001, epoch=25, disp=False):\n",
    "    \"\"\" This function trains a model given the training input data\n",
    "        and training target data.\n",
    "        args:\n",
    "            model: model to train\n",
    "            train_input: training input data\n",
    "            train_target: training target data\n",
    "        returns:\n",
    "            loss_arr: an array containing the training loss for each epoch\n",
    "            eval_arr: an array containing the evaluation loss for each epoch\n",
    "    \"\"\"\n",
    "    loss_arr = []\n",
    "    eval_arr = []\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=eta)\n",
    "    for e in range(epoch):\n",
    "        sum_loss = 0\n",
    "        for input_, target_ in zip(train_input.split(mini_batch_size),\n",
    "                                  train_target.split(mini_batch_size)):\n",
    "            output = model(input_)\n",
    "            loss = criterion(output, target_)\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            sum_loss += loss.item()\n",
    "            optimizer.step()\n",
    "            \n",
    "        loss_eval = eval_loss(model, test_input, test_target)\n",
    "        if((e % 2 == 0) and disp):\n",
    "            print(\"Epoch: {}\\tLoss out: {}\\tLoss eval: {}\".format(e, sum_loss, loss_eval))\n",
    "        \n",
    "        loss_arr.append(sum_loss)\n",
    "        eval_arr.append(loss_eval)\n",
    "        \n",
    "    return loss_arr, eval_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_comp(intermed, output_net, train_input, train_target, train_classes,\n",
    "                     test_input, test_target, test_classes,\n",
    "                     eta=0.001, mini_batch_size=25, epoch1=25, epoch2=25, lr1=0.001, lr2=0.001, disp=False):\n",
    "    \"\"\" This function trains a composite model given the training input data,\n",
    "        the training target data and the number of training classes.\n",
    "        args:\n",
    "            intermed: intermediate network\n",
    "            output_net: output network\n",
    "            train_input: training input data\n",
    "            train_target: training target data\n",
    "            train_classes: number of classes in the training data\n",
    "        returns:\n",
    "            loss_arr: an array containing the loss for each epoch\n",
    "            eval_arr: an array containing the evaluation loss for each epoch\n",
    "    \"\"\"\n",
    "    loss_arr = []\n",
    "    eval_arr = []\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optim1 = optim.Adam(intermed.parameters(), lr=lr1)\n",
    "    optim2 = optim.Adam(output_net.parameters(), lr=lr2)\n",
    "    # Train the intermediate\n",
    "    for e in range(epoch1):\n",
    "        sum_loss = 0\n",
    "        for input_, class_ in zip(train_input.split(mini_batch_size), \n",
    "                                   train_classes.split(mini_batch_size)):\n",
    "            out1, out2 = intermed(input_)\n",
    "            loss = criterion(out1, class_[:,0]) + criterion(out2, class_[:,1])\n",
    "            intermed.zero_grad()\n",
    "            loss.backward()\n",
    "            sum_loss += loss.item()\n",
    "            optim1.step()\n",
    "        if((e % 2 == 0) & disp):\n",
    "            print(\"Epoch: {}\\tLoss inter train: {}\\tLoss inter eval: {}\\t\".format(e, sum_loss,\n",
    "                                                                                  eval_loss_inter(intermed,\n",
    "                                                                                                  test_input,\n",
    "                                                                                                  test_classes)))\n",
    "    # Train the output\n",
    "    for e in range(epoch2):\n",
    "        sum_loss = 0\n",
    "        for input_, target_ in zip(train_input.split(mini_batch_size), \n",
    "                                   train_target.split(mini_batch_size)):\n",
    "            out1, out2 = intermed(input_) \n",
    "            nums = torch.cat([out1, out2],1)\n",
    "            output = output_net(nums)\n",
    "            loss = criterion(output, target_)\n",
    "            output_net.zero_grad()\n",
    "            loss.backward()\n",
    "            sum_loss += loss.item()\n",
    "            optim2.step()\n",
    "        \n",
    "        loss_eval = eval_loss(Composite(intermed, output_net), test_input, test_target) \n",
    "        if(disp):\n",
    "            print(\"Epoch\", e, \"Loss out:\", sum_loss, \"Loss eval: \", loss_eval)\n",
    "        loss_arr.append(sum_loss)\n",
    "        eval_arr.append(loss_eval)\n",
    "    return loss_arr, eval_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nb_errors(model, input_data, target_data):\n",
    "    \"\"\" This function computes the number of errors given a trained model,\n",
    "        test input and corresponding test output.\n",
    "        args: \n",
    "            model: model trained\n",
    "            input: test input\n",
    "            target: test output\n",
    "        return:\n",
    "            nb_errors: the number of misclassified samples\n",
    "    \"\"\"\n",
    "    nb_errors = 0\n",
    "    output = model(input_data)\n",
    "    _, predicted_classes = output.max(1)\n",
    "    nb_errors = (target_data != predicted_classes).sum().item()\n",
    "    return nb_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Networks implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Net - Simple network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, nb_hidden=100):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(2, 16, kernel_size=3)    #2*14*14 --> 16*12*12\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3)   #16*10*10 --> 32*8*8\n",
    "        self.fc1 = nn.Linear(288, nb_hidden)\n",
    "        self.fc2 = nn.Linear(nb_hidden, 2)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=3, stride=1)) #16*12*12 --> 16*10*10\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=3, stride=2)) #32*8*8 --> 32*3*3\n",
    "        x = F.relu(self.fc1(x.view(-1, 288)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "print('Number of parameters:', sum(p.numel() for p in Net().parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "losses = []\n",
    "evals = []\n",
    "nb_errors = []\n",
    "\n",
    "for i in range(n_rounds):\n",
    "    # Train the model\n",
    "    model = Net()\n",
    "    l, e = train_model(model, train_input_norm, train_target, test_input_norm, test_target)\n",
    "    losses.append(l)\n",
    "    evals.append(e)\n",
    "    # Compute the number of errors\n",
    "    nb_errors.append(compute_nb_errors(model, test_input_norm, test_target))\n",
    "\n",
    "nb_params.append(sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "losses_mean.append(np.mean(losses, axis=0))\n",
    "eval_mean.append(np.mean(evals, axis=0))\n",
    "nb_errors_mean.append(np.mean(nb_errors, axis=0))\n",
    "losses_std.append(np.std(losses, axis=0))\n",
    "eval_std.append(np.std(evals, axis=0))\n",
    "nb_errors_std.append(np.std(nb_errors, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Net2 - More complex network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net2(nn.Module):\n",
    "    def __init__(self, nb_hidden=200):\n",
    "        super(Net2, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(2, 16, kernel_size=3)  #2*14*14 --> 16*12*12\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3) #16*10*10 --> 32*8*8\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3) #32*6*6 --> 64*4*4\n",
    "        self.fc1 = nn.Linear(16 * 64, nb_hidden)\n",
    "        self.fc2 = nn.Linear(nb_hidden, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=3, stride=1)) #16*12*12 --> 16*10*10\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=3, stride=1)) #32*8*8 --> 32*6*6\n",
    "        x = F.relu(self.conv3(x))                                        #64*4*4v\n",
    "        x = F.relu(self.fc1(x.view(-1, 16 * 64)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "print('Number of parameters:', sum(p.numel() for p in Net2().parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "losses = []\n",
    "evals = []\n",
    "nb_errors = []\n",
    "\n",
    "for i in range(n_rounds):\n",
    "    # Train the model\n",
    "    model = Net2()\n",
    "    l, e = train_model(model, train_input_norm, train_target, test_input_norm, test_target)\n",
    "    losses.append(l)\n",
    "    evals.append(e)\n",
    "    # Compute the number of errors\n",
    "    nb_errors.append(compute_nb_errors(model, test_input_norm, test_target))\n",
    "\n",
    "nb_params.append(sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "losses_mean.append(np.mean(losses, axis=0))\n",
    "eval_mean.append(np.mean(evals, axis=0))\n",
    "nb_errors_mean.append(np.mean(nb_errors, axis=0))\n",
    "losses_std.append(np.std(losses, axis=0))\n",
    "eval_std.append(np.std(evals, axis=0))\n",
    "nb_errors_std.append(np.std(nb_errors, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Composite - Use auxiliary loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntermediateNet(nn.Module):\n",
    "    def __init__(self, nb_hidden=1000):\n",
    "        super(IntermediateNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)  #2*14*14 --> 32*12*12\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3) #32*10*10 --> 64*8*8\n",
    "        self.fc1 = nn.Linear(64*9, nb_hidden)\n",
    "        self.out = nn.Linear(nb_hidden, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1, x2 = x[:,0].unsqueeze(1), x[:,1].unsqueeze(1)\n",
    "        x1 = F.relu(F.max_pool2d(self.conv1(x1), kernel_size=3, stride=1)) #32*12*12 --> 32*10*10\n",
    "        x1 = F.relu(F.max_pool2d(self.conv2(x1), kernel_size=3, stride=2)) #64*8*8 --> 64*3*3\n",
    "        x1 = F.relu(self.fc1(x1.view(-1, 64*9)))\n",
    "        y1 = self.out(x1)\n",
    "        \n",
    "        x2 = F.relu(F.max_pool2d(self.conv1(x2), kernel_size=3, stride=1)) #32*12*12 --> 32*10*10\n",
    "        x2 = F.relu(F.max_pool2d(self.conv2(x2), kernel_size=3, stride=2)) #64*8*8 --> 64*3*3\n",
    "        x2 = F.relu(self.fc1(x2.view(-1, 64*9)))\n",
    "        y2 = self.out(x2)\n",
    "        return y1, y2\n",
    "    \n",
    "class OutputNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OutputNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(20, 50)\n",
    "        self.fc2 = nn.Linear(50,2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "class Composite(nn.Module):\n",
    "    def __init__(self, intermediate, final):\n",
    "        super(Composite, self).__init__()\n",
    "        self.intermediate = intermediate\n",
    "        self.final = final\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y1, y2 = self.intermediate(x)\n",
    "        return self.final(torch.cat([y1,y2], 1))\n",
    "    \n",
    "print('Number of parameters:', sum(p.numel() for p in Composite(IntermediateNet(), OutputNet()).parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "losses = []\n",
    "evals = []\n",
    "nb_errors = []\n",
    "\n",
    "for i in range(n_rounds):\n",
    "    # Train the model\n",
    "    inter = IntermediateNet()\n",
    "    out = OutputNet()\n",
    "    eta = 0.001\n",
    "    l, e = train_model_comp(inter, out, train_input_norm, train_target, train_classes, test_input_norm,\n",
    "                            test_target, test_classes, eta, epoch1=25, epoch2=25)\n",
    "    losses.append(l)\n",
    "    evals.append(e)\n",
    "    # Compute the number of errors\n",
    "    model = Composite(inter, out)\n",
    "    nb_errors.append(compute_nb_errors(model, test_input_norm, test_target))\n",
    "\n",
    "nb_params.append(sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "losses_mean.append(np.mean(losses, axis=0))\n",
    "eval_mean.append(np.mean(evals, axis=0))\n",
    "nb_errors_mean.append(np.mean(nb_errors, axis=0))\n",
    "losses_std.append(np.std(losses, axis=0))\n",
    "eval_std.append(np.std(evals, axis=0))\n",
    "nb_errors_std.append(np.std(nb_errors, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetBlock(nn.Module):\n",
    "    def __init__(self, nb_channels, kernel_size, skip_connections, batch_normalization):\n",
    "        super(ResNetBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(nb_channels, nb_channels, kernel_size=kernel_size, padding=(kernel_size - 1) // 2)\n",
    "        self.bn1 = nn.BatchNorm2d(nb_channels)\n",
    "        self.conv2 = nn.Conv2d(nb_channels, nb_channels, kernel_size=kernel_size, padding=(kernel_size - 1) // 2)\n",
    "        self.bn2 = nn.BatchNorm2d(nb_channels)\n",
    "        \n",
    "        self.skip_connections = skip_connections\n",
    "        self.batch_normalization = batch_normalization\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.conv1(x)\n",
    "        if self.batch_normalization:\n",
    "            y = self.bn1(y)\n",
    "        y = F.relu(y)\n",
    "        y = self.conv2(y)\n",
    "        if self.batch_normalization:\n",
    "            y = self.bn2(y)\n",
    "        if self.skip_connections:\n",
    "            y = y + x\n",
    "        y = F.relu(y)\n",
    "        return y\n",
    "    \n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, nb_residual_blocks, nb_channels, kernel_size=3, nb_classes=10,\n",
    "                 skip_connections=True, batch_normalization=True):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv = nn.Conv2d(2, nb_channels, kernel_size=kernel_size, padding=(kernel_size - 1) // 2)\n",
    "        self.bn = nn.BatchNorm2d(nb_channels)\n",
    "        self.resnet_blocks = nn.Sequential(\n",
    "            *(ResNetBlock(nb_channels, kernel_size, skip_connections, batch_normalization)\n",
    "              for _ in range(nb_residual_blocks)))\n",
    "        self.fc1 = nn.Linear(nb_channels, nb_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn(self.conv(x)))\n",
    "        x = self.resnet_blocks(x)\n",
    "        x = F.avg_pool2d(x, 14).view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "losses = []\n",
    "evals = []\n",
    "nb_errors = []\n",
    "\n",
    "for i in range(n_rounds):\n",
    "    # Train the model\n",
    "    model = ResNet(nb_residual_blocks=2, nb_channels=16, kernel_size=3, nb_classes=2,\n",
    "                   skip_connections=True, batch_normalization=True)\n",
    "    l, e = train_model(model, train_input_norm, train_target, test_input_norm, test_target)\n",
    "    losses.append(l)\n",
    "    evals.append(e)\n",
    "    # Compute the number of errors\n",
    "    nb_errors.append(compute_nb_errors(model, test_input_norm, test_target))\n",
    "\n",
    "nb_params.append(sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "losses_mean.append(np.mean(losses, axis=0))\n",
    "eval_mean.append(np.mean(evals, axis=0))\n",
    "nb_errors_mean.append(np.mean(nb_errors, axis=0))\n",
    "losses_std.append(np.std(losses, axis=0))\n",
    "eval_std.append(np.std(evals, axis=0))\n",
    "nb_errors_std.append(np.std(nb_errors, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 ResNet - Composite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntermediateResNet(nn.Module):\n",
    "    def __init__(self, nb_residual_blocks, nb_channels, kernel_size=3, nb_classes=10,\n",
    "                 skip_connections=True, batch_normalization=True, nb_hidden=1000):\n",
    "        super(IntermediateResNet, self).__init__()\n",
    "        self.conv = nn.Conv2d(1, nb_channels, kernel_size=kernel_size, padding=(kernel_size - 1) // 2)\n",
    "        self.bn = nn.BatchNorm2d(nb_channels)\n",
    "        self.resnet_blocks = nn.Sequential(\n",
    "            *(ResNetBlock(nb_channels, kernel_size, skip_connections, batch_normalization)\n",
    "              for _ in range(nb_residual_blocks)))\n",
    "        self.fc1 = nn.Linear(nb_channels, nb_hidden)\n",
    "        self.out = nn.Linear(nb_hidden, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1, x2 = x[:,0].unsqueeze(1), x[:,1].unsqueeze(1)\n",
    "        \n",
    "        x1 = F.relu(self.bn(self.conv(x1)))\n",
    "        x1 = self.resnet_blocks(x1)\n",
    "        x1 = F.avg_pool2d(x1, 14).view(x1.size(0), -1)\n",
    "        x1 = self.fc1(x1)\n",
    "        y1 = self.out(x1)\n",
    "        \n",
    "        x2 = F.relu(self.bn(self.conv(x2)))\n",
    "        x2 = self.resnet_blocks(x2)\n",
    "        x2 = F.avg_pool2d(x2, 14).view(x2.size(0), -1)\n",
    "        x2 = self.fc1(x2)\n",
    "        y2 = self.out(x2)\n",
    "        return y1, y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "nb_errors = []\n",
    "evals = []\n",
    "\n",
    "for i in range(n_rounds):\n",
    "    # Train the model\n",
    "    inter = IntermediateResNet(nb_residual_blocks=2, nb_channels=16, nb_classes=10)\n",
    "    out = OutputNet()\n",
    "    eta = 0.001\n",
    "    l, e = train_model_comp(inter, out, train_input_norm, train_target, train_classes,\n",
    "                                   test_input_norm, test_target, test_classes, eta)\n",
    "    losses.append(l)\n",
    "    evals.append(e)\n",
    "    # Compute the number of errors\n",
    "    model = Composite(inter, out)\n",
    "    nb_errors.append(compute_nb_errors(model, test_input_norm, test_target))\n",
    "\n",
    "nb_params.append(sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "losses_mean.append(np.mean(losses, axis=0))\n",
    "eval_mean.append(np.mean(evals, axis=0))\n",
    "nb_errors_mean.append(np.mean(nb_errors, axis=0))\n",
    "losses_std.append(np.std(losses, axis=0))\n",
    "eval_std.append(np.std(evals, axis=0))\n",
    "nb_errors_std.append(np.std(nb_errors, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3. Model comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "labels = ['Net', 'Net2', 'CompositeNet', 'ResNet', 'CompositeResNet']\n",
    "colors = ['peru', 'maroon', 'forestgreen', 'steelblue', 'darkorange']\n",
    "fig, ax = plt.subplots(4, 1, figsize=(16, 40))\n",
    "n_epochs = 25\n",
    "\n",
    "for i in range(5):\n",
    "    X = [x for x in range(n_epochs)]\n",
    "    ax[0].errorbar(X, losses_mean[i], yerr=losses_std[i], fmt='.-',\n",
    "                   label=labels[i], color=colors[i])\n",
    "ax[0].legend()\n",
    "ax[0].set_title('Mean training loss over {} rounds of {} epochs with standard deviation'.format(n_rounds, n_epochs))\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].grid(True)\n",
    "\n",
    "for i in range(5):\n",
    "    X = [x for x in range(n_epochs)]\n",
    "    ax[1].errorbar(X, eval_mean[i], yerr=eval_std[i], fmt='.-',\n",
    "                   label=labels[i], color=colors[i])\n",
    "ax[1].legend()\n",
    "ax[1].set_title('Mean evaluation loss over {} rounds of {} epochs with standard deviation'.format(n_rounds, n_epochs))\n",
    "ax[1].set_xlabel('Epoch')\n",
    "ax[1].set_ylabel('Loss')\n",
    "ax[1].grid(True)\n",
    "\n",
    "ax[2].bar(labels, list(map(lambda x: x/1000*100, nb_errors_mean)), color=colors, width=0.25,\n",
    "          yerr=list(map(lambda x: x/1000 * 100, nb_errors_std)))\n",
    "ax[2].set_title('Mean percentage of errors over {} rounds with standard deviation'.format(n_rounds))\n",
    "ax[2].set_ylabel('Errors')\n",
    "ax[2].yaxis.grid(True)\n",
    "\n",
    "ax[3].bar(labels, nb_params, color=colors, width=0.25)\n",
    "ax[3].set_title('Number of parameters')\n",
    "ax[3].set_ylabel('Number of parameters')\n",
    "ax[3].yaxis.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
