{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2\n",
    "Martin Esguerra, Leo Bouraux, Franck Dessimoz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, time\n",
    "import sys\n",
    "import math\n",
    "torch.set_grad_enabled( False );\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "Your framework must provide the necessary tools to:\n",
    "- build networks combining fully connected layers, Tanh, and ReLU,\n",
    "- run the forward and backward passes,\n",
    "- optimize parameters with SGD for MSE.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_disc_set(nb):\n",
    "    input = torch.Tensor(nb, 2).uniform_(-1, 1)\n",
    "    target = input.pow(2).sum(1).sub(2 / math.pi).sign().add(1).div(2).float()\n",
    "    return input, target\n",
    "\n",
    "train_input, train_target = generate_disc_set(1000)\n",
    "test_input, test_target = generate_disc_set(1000)\n",
    "\n",
    "mean, std = train_input.mean(), train_input.std()\n",
    "\n",
    "train_input.sub_(mean).div_(std)\n",
    "test_input.sub_(mean).div_(std)\n",
    "\n",
    "mini_batch_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module Structure\n",
    "Implement multiple modules, starting with linear\n",
    "\n",
    "Look pytorch https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input has batch dim in entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module ( object ) :\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "    def forward ( self , * input ) :\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def backward ( self , * gradwrtoutput ) :\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def param ( self ) :\n",
    "        return self.params\n",
    "    \n",
    "    def __call__(self, input):\n",
    "        return self.forward(input)\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear ( Module ) :\n",
    "    def __init__(self, in_features, out_features, epsilon=1e-6, bias=True):\n",
    "        super(Linear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        stdv = 1. / math.sqrt(in_features)\n",
    "\n",
    "        self.weight = torch.empty(in_features, out_features).uniform_(-stdv, stdv)\n",
    "        self.grad_w = torch.empty(self.weight.size())\n",
    "        self.params.append((self.weight, self.grad_w))\n",
    "        if bias:\n",
    "            self.bias = torch.empty(out_features).uniform_(-stdv, stdv)\n",
    "            self.grad_b = torch.empty(out_features)\n",
    "            self.params.append((self.bias, self.grad_b))\n",
    "        else:\n",
    "            self.bias = None\n",
    "        self.curr_x = torch.empty(in_features)\n",
    "        \n",
    "    def forward ( self , input ) :\n",
    "        self.curr_x = input\n",
    "        output = input.matmul(self.weight)\n",
    "        if self.bias is not None:\n",
    "            output += self.bias\n",
    "        return output\n",
    "        \n",
    "    def backward ( self , gradwrtoutput ) :\n",
    "    # TODO NEEDs to be checked\n",
    "    # need to put to zero after each iteration of samples\n",
    "        self.grad_w.add_(self.curr_x.t().matmul(gradwrtoutput))\n",
    "        if self.bias is not None:\n",
    "            self.grad_b.add_(gradwrtoutput.sum(dim=0))\n",
    "        return gradwrtoutput.matmul(self.weight.t())\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        self.grad_w.zero_()\n",
    "        self.grad_b.zero_()\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh ( Module ) :\n",
    "    def __init__(self):\n",
    "        super(Tanh, self).__init__()\n",
    "        self.x = None\n",
    "    def forward ( self , input ) :\n",
    "        self.x = input.clone()\n",
    "        return input.tanh()\n",
    "    \n",
    "    def backward ( self , gradwrtoutput ) :\n",
    "        return 4 * (self.x.exp() + self.x.mul(-1).exp()).pow(-2) * gradwrtoutput\n",
    "    \n",
    "    def param ( self ) :\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU ( Module ) :\n",
    "    def __init__(self):\n",
    "        super(ReLU, self).__init__()\n",
    "        self.x = None\n",
    "    def forward ( self ,  input ) :\n",
    "        \n",
    "        if not isinstance(input, torch.Tensor):\n",
    "            raise Exception(\"Wrong input type\")\n",
    "        out = input.clone()\n",
    "        out[out < 0] = 0\n",
    "        self.x = out.clone()\n",
    "        return out\n",
    "    \n",
    "    def backward ( self , gradwrtoutput ) :\n",
    "        if len(self.x) == 0:\n",
    "            return None\n",
    "        grad_in = gradwrtoutput.clone()\n",
    "        grad_in[self.x < 0] = 0\n",
    "        return grad_in\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossMSE ( Module ) :\n",
    "    def __init__(self):\n",
    "        super(LossMSE, self).__init__()\n",
    "        self.diff = None\n",
    "        \n",
    "    def forward ( self , input, target ) :\n",
    "        if target.dim() ==1:\n",
    "            target = target.view(-1,1)\n",
    "        self.diff = input - target\n",
    "        \n",
    "        return (input - target).pow(2).sum()/ 2\n",
    "    \n",
    "    def backward ( self) :\n",
    "        return self.diff\n",
    "    \n",
    "    def __call__(self, inp, t):\n",
    "        return self.forward(inp, t)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential ( Module ) :\n",
    "    def __init__(self, modules):\n",
    "        super(Sequential, self).__init__()\n",
    "        self.modules = modules\n",
    "        for mod in modules:\n",
    "            if mod.param():\n",
    "                self.params += mod.param()\n",
    "    def forward ( self , input ) :\n",
    "        y = input\n",
    "        for mod in self.modules:\n",
    "            y = mod.forward(y)\n",
    "        return y\n",
    "    \n",
    "    def backward ( self ,  gradwrtoutput ) :\n",
    "        grad_in = gradwrtoutput.clone()\n",
    "        for mod in self.modules[::-1]:\n",
    "            grad_in = mod.backward(grad_in)\n",
    "        return grad_in\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        for mod in self.modules:\n",
    "            mod.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(object):\n",
    "    def __init__(self, params, lr=0.001):\n",
    "        self.params = params\n",
    "        self.lr=lr\n",
    "        \n",
    "    def step(self):\n",
    "        for p, grad in self.params:\n",
    "            p -= self.lr * grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 158.5514349937439\n",
      "1 115.5336332321167\n",
      "2 112.75693559646606\n",
      "3 111.08500576019287\n",
      "4 109.54303216934204\n",
      "5 108.00308275222778\n",
      "6 106.43621969223022\n",
      "7 104.80847501754761\n",
      "8 103.15799283981323\n",
      "9 101.45624923706055\n",
      "10 99.70861482620239\n",
      "11 97.95834302902222\n",
      "12 96.19540643692017\n",
      "13 94.41789484024048\n",
      "14 92.64863061904907\n",
      "15 90.8865475654602\n",
      "16 89.12988567352295\n",
      "17 87.38075280189514\n",
      "18 85.660227060318\n",
      "19 83.98171401023865\n",
      "20 82.33645701408386\n",
      "21 80.72172236442566\n",
      "22 79.15902304649353\n",
      "23 77.6590473651886\n",
      "24 76.22169995307922\n"
     ]
    }
   ],
   "source": [
    "a,b,c,d = Linear(2,10), ReLU(), Linear(10,1), Tanh()\n",
    "model = Sequential([a,b,c,d])\n",
    "#model = Linear(2,1)\n",
    "train_model_2(model, train_input, train_target, it=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_input, train_target, eta=1e-1, it=10):\n",
    "    mse = LossMSE()\n",
    "    optim = SGD(model.param())\n",
    "    for e in range(it):\n",
    "        model.zero_grad()\n",
    "        sum_loss = 0\n",
    "        for b in range(train_input.size(0)):            \n",
    "            output = model(train_input[b].unsqueeze(0))\n",
    "            loss = mse(output, train_target[b])\n",
    "            grad_l = mse.backward()\n",
    "            model.backward(grad_l)\n",
    "            sum_loss = sum_loss + loss.item()\n",
    "        \n",
    "        optim.step()\n",
    "        print(e, sum_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use if model can handle minibatch\n",
    "def train_model_2(model, train_input, train_target, eta=1e-3, mini_batch_size=50, it=20):\n",
    "    mse = LossMSE()\n",
    "    optim = SGD(model.param())\n",
    "    for e in range(it):\n",
    "        sum_loss = 0\n",
    "        for b in range(0, train_input.size(0), mini_batch_size):\n",
    "            output = model(train_input.narrow(0, b, mini_batch_size))\n",
    "            loss = mse(output, train_target.narrow(0, b, mini_batch_size))\n",
    "            \n",
    "            model.zero_grad()\n",
    "            grad_l = mse.backward()\n",
    "            model.backward(grad_l)\n",
    "            sum_loss = sum_loss + loss.item()\n",
    "            optim.step()\n",
    "        print(e, sum_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
