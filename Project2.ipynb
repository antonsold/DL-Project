{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x17fce9f4df0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "torch.set_grad_enabled(False)\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module(object):\n",
    "    def __init__(self):\n",
    "        self.parameters = []\n",
    "        \n",
    "    def forward(self, *input):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def backward(self, *gradwrtoutput):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def param(self):\n",
    "        return self.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        init_range = 1. / math.sqrt(in_features)\n",
    "        self.weights = torch.Tensor(in_features, out_features).uniform_(-init_range, init_range)\n",
    "        self.grad_w = torch.zeros((in_features, out_features))\n",
    "        self.parameters = [(self.weights, self.grad_w)]\n",
    "        if bias:\n",
    "            self.bias = torch.Tensor(out_features).uniform_(-init_range, init_range)\n",
    "            self.grad_b = torch.zeros(out_features)\n",
    "            self.parameters.append((self.bias, self.grad_b))\n",
    "        else:\n",
    "            self.bias = None\n",
    "        \n",
    "    def forward(self, input_):\n",
    "        self.input = input_\n",
    "        if self.bias != None:\n",
    "            return torch.addmm(self.bias, input_, self.weights)\n",
    "        else:\n",
    "            return input_.matmul(self.weights)\n",
    "        \n",
    "    def backward(self, grad_output):\n",
    "        self.grad_w += self.input.t().matmul(grad_output)\n",
    "        grad_input = grad_output.matmul(self.weights.t())\n",
    "        if self.bias != None:\n",
    "            self.grad_b += grad_output.sum(dim=0)\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "    def forward(self, input_):\n",
    "        self.input = input_\n",
    "        return torch.relu(input_)\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        return torch.mul((self.input > 0).int(), grad_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh(Module):\n",
    "    def forward(self, input_):\n",
    "        self.input = input_\n",
    "        return torch.tanh(input_)\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        return torch.tanh(self.input).pow(2).mul(-1).add(1).mul(grad_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(Module):\n",
    "    def __init__(self, *args):\n",
    "        self.layers = list(args)\n",
    "        self.parameters = []\n",
    "        for module in args:\n",
    "            self.parameters += module.parameters\n",
    "    \n",
    "    def forward(self, input_):\n",
    "        x = input_\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def backward(self, loss_grad):\n",
    "        y = loss_grad\n",
    "        for layer in reversed(self.layers):\n",
    "            y = layer.backward(y)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSELoss(Module):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, input_, target):\n",
    "        return self.forward(input_, target)\n",
    "    \n",
    "    def forward(self, input_, target):\n",
    "        if target.dim() == 1:\n",
    "            target = target.view(target.size(0), 1)\n",
    "        return (input_ - target).pow(2).sum().item()\n",
    "    \n",
    "    def backward(self, input_, target):\n",
    "        if target.dim() == 1:\n",
    "            target = target.view(target.size(0), 1)\n",
    "        \n",
    "        return (input_ - target).mul(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_set(size):\n",
    "    input_ = torch.Tensor(size, 2).uniform_(0, 1)\n",
    "    target_single = input_.sub(0.5).pow(2).sum(axis=1).sub(1 / (math.pi * 2)).sign().view(-1, 1)\n",
    "    target = torch.cat((target_single.mul(-1), target_single), 1).add(1).div(2)\n",
    "    return input_, target\n",
    "\n",
    "train_input, train_target = generate_set(sample_size)\n",
    "test_input, test_target = generate_set(sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "        self.parameters = []\n",
    "        for layer in layers:\n",
    "            self.parameters += layer.parameters\n",
    "    \n",
    "    def __call__(self, input_):\n",
    "        return self.forward(input_)\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        for w, dw in self.parameters:\n",
    "            dw.zero_()\n",
    "    \n",
    "    def forward(self, input_):\n",
    "        x = input_\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def backward(self, loss_grad):\n",
    "        y = loss_grad\n",
    "        for layer in reversed(self.layers):\n",
    "            y = layer.backward(y)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, parameters, learning_rate):\n",
    "        self.parameters = parameters\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def step(self):\n",
    "        for w, dw in self.parameters:\n",
    "            w.sub_(self.learning_rate * dw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [Linear(2, 25), ReLU(), Linear(25, 25), ReLU(), Linear(25, 25), ReLU(), Linear(25, 2), Tanh()]\n",
    "model = Model(layers)\n",
    "loss = MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_input, train_target, batch_size=100, n_epochs=100, loss=MSELoss(), learning_rate=0.001, print_loss=True):\n",
    "    sample_size = train_input.size(0)\n",
    "    print(sample_size)\n",
    "    sgd = SGD(model.parameters, learning_rate)\n",
    "    for epoch in range(n_epochs):\n",
    "        cumulative_loss = 0\n",
    "        for n_start in range(0, sample_size, batch_size):\n",
    "            model.zero_grad()\n",
    "            output = model(train_input[n_start : n_start + batch_size])\n",
    "            cumulative_loss += loss(output, train_target[n_start : n_start + batch_size])\n",
    "            loss_grad = loss.backward(output, train_target[n_start : n_start + batch_size])\n",
    "            model.backward(loss_grad)\n",
    "            sgd.step()\n",
    "        if print_loss:\n",
    "            print(\"Epoch: %i\" % epoch)\n",
    "            print(\"Loss: %f\" % cumulative_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "Epoch: 0\n",
      "Loss: 87.831915\n",
      "Epoch: 1\n",
      "Loss: 89.499357\n",
      "Epoch: 2\n",
      "Loss: 87.446156\n",
      "Epoch: 3\n",
      "Loss: 87.051387\n",
      "Epoch: 4\n",
      "Loss: 85.301146\n",
      "Epoch: 5\n",
      "Loss: 85.314953\n",
      "Epoch: 6\n",
      "Loss: 85.250834\n",
      "Epoch: 7\n",
      "Loss: 84.456578\n",
      "Epoch: 8\n",
      "Loss: 83.065378\n",
      "Epoch: 9\n",
      "Loss: 82.181355\n",
      "Epoch: 10\n",
      "Loss: 79.679480\n",
      "Epoch: 11\n",
      "Loss: 81.547590\n",
      "Epoch: 12\n",
      "Loss: 81.215204\n",
      "Epoch: 13\n",
      "Loss: 79.794634\n",
      "Epoch: 14\n",
      "Loss: 80.366846\n",
      "Epoch: 15\n",
      "Loss: 79.551367\n",
      "Epoch: 16\n",
      "Loss: 78.953803\n",
      "Epoch: 17\n",
      "Loss: 79.431170\n",
      "Epoch: 18\n",
      "Loss: 76.218543\n",
      "Epoch: 19\n",
      "Loss: 77.240423\n",
      "Epoch: 20\n",
      "Loss: 75.720995\n",
      "Epoch: 21\n",
      "Loss: 77.332279\n",
      "Epoch: 22\n",
      "Loss: 76.078060\n",
      "Epoch: 23\n",
      "Loss: 73.418399\n",
      "Epoch: 24\n",
      "Loss: 75.921631\n",
      "Epoch: 25\n",
      "Loss: 73.486027\n",
      "Epoch: 26\n",
      "Loss: 71.474735\n",
      "Epoch: 27\n",
      "Loss: 71.287388\n",
      "Epoch: 28\n",
      "Loss: 71.462109\n",
      "Epoch: 29\n",
      "Loss: 73.669683\n",
      "Epoch: 30\n",
      "Loss: 65.930800\n",
      "Epoch: 31\n",
      "Loss: 71.172777\n",
      "Epoch: 32\n",
      "Loss: 68.472447\n",
      "Epoch: 33\n",
      "Loss: 68.141115\n",
      "Epoch: 34\n",
      "Loss: 67.151998\n",
      "Epoch: 35\n",
      "Loss: 67.491766\n",
      "Epoch: 36\n",
      "Loss: 66.161403\n",
      "Epoch: 37\n",
      "Loss: 66.764684\n",
      "Epoch: 38\n",
      "Loss: 65.378579\n",
      "Epoch: 39\n",
      "Loss: 67.592018\n",
      "Epoch: 40\n",
      "Loss: 66.754793\n",
      "Epoch: 41\n",
      "Loss: 62.339128\n",
      "Epoch: 42\n",
      "Loss: 64.224596\n",
      "Epoch: 43\n",
      "Loss: 64.088507\n",
      "Epoch: 44\n",
      "Loss: 61.420132\n",
      "Epoch: 45\n",
      "Loss: 59.703152\n",
      "Epoch: 46\n",
      "Loss: 63.213950\n",
      "Epoch: 47\n",
      "Loss: 56.492472\n",
      "Epoch: 48\n",
      "Loss: 57.985948\n",
      "Epoch: 49\n",
      "Loss: 62.501696\n",
      "Epoch: 50\n",
      "Loss: 56.811744\n",
      "Epoch: 51\n",
      "Loss: 53.778368\n",
      "Epoch: 52\n",
      "Loss: 60.308465\n",
      "Epoch: 53\n",
      "Loss: 59.045905\n",
      "Epoch: 54\n",
      "Loss: 57.419858\n",
      "Epoch: 55\n",
      "Loss: 56.935038\n",
      "Epoch: 56\n",
      "Loss: 58.961245\n",
      "Epoch: 57\n",
      "Loss: 56.149358\n",
      "Epoch: 58\n",
      "Loss: 54.730138\n",
      "Epoch: 59\n",
      "Loss: 56.236119\n",
      "Epoch: 60\n",
      "Loss: 56.810182\n",
      "Epoch: 61\n",
      "Loss: 56.743468\n",
      "Epoch: 62\n",
      "Loss: 52.315403\n",
      "Epoch: 63\n",
      "Loss: 57.644393\n",
      "Epoch: 64\n",
      "Loss: 57.670950\n",
      "Epoch: 65\n",
      "Loss: 57.264122\n",
      "Epoch: 66\n",
      "Loss: 58.369486\n",
      "Epoch: 67\n",
      "Loss: 57.571620\n",
      "Epoch: 68\n",
      "Loss: 51.091870\n",
      "Epoch: 69\n",
      "Loss: 48.902480\n",
      "Epoch: 70\n",
      "Loss: 54.243150\n",
      "Epoch: 71\n",
      "Loss: 52.943598\n",
      "Epoch: 72\n",
      "Loss: 51.950122\n",
      "Epoch: 73\n",
      "Loss: 52.759941\n",
      "Epoch: 74\n",
      "Loss: 43.081946\n",
      "Epoch: 75\n",
      "Loss: 42.342674\n",
      "Epoch: 76\n",
      "Loss: 42.022229\n",
      "Epoch: 77\n",
      "Loss: 49.489403\n",
      "Epoch: 78\n",
      "Loss: 57.297225\n",
      "Epoch: 79\n",
      "Loss: 59.950151\n",
      "Epoch: 80\n",
      "Loss: 52.642969\n",
      "Epoch: 81\n",
      "Loss: 51.160185\n",
      "Epoch: 82\n",
      "Loss: 40.361059\n",
      "Epoch: 83\n",
      "Loss: 43.065904\n",
      "Epoch: 84\n",
      "Loss: 42.602260\n",
      "Epoch: 85\n",
      "Loss: 39.025472\n",
      "Epoch: 86\n",
      "Loss: 38.696007\n",
      "Epoch: 87\n",
      "Loss: 45.612110\n",
      "Epoch: 88\n",
      "Loss: 60.541399\n",
      "Epoch: 89\n",
      "Loss: 53.501669\n",
      "Epoch: 90\n",
      "Loss: 41.073595\n",
      "Epoch: 91\n",
      "Loss: 59.860205\n",
      "Epoch: 92\n",
      "Loss: 42.601484\n",
      "Epoch: 93\n",
      "Loss: 62.769444\n",
      "Epoch: 94\n",
      "Loss: 37.980381\n",
      "Epoch: 95\n",
      "Loss: 44.999155\n",
      "Epoch: 96\n",
      "Loss: 58.428993\n",
      "Epoch: 97\n",
      "Loss: 46.185990\n",
      "Epoch: 98\n",
      "Loss: 64.577121\n",
      "Epoch: 99\n",
      "Loss: 36.804721\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_input, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(true_target, predicted):\n",
    "    return true_target.argmax(dim=1).sub(predicted.argmax(dim=1)).eq(0).float().mean().item()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
